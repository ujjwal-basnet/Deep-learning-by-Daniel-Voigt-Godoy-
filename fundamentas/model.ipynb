{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cofiguration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.b = nn.Parameter(torch.randn(1 , \n",
    "                                          requires_grad= True ,  \n",
    "                                          dtype = torch.float ))\n",
    "        \n",
    "        self.w = nn.Parameter(torch.randn(1 , requires_grad= True ,\n",
    "                                          dtype = torch.float ))\n",
    "\n",
    "    def forward(self ,x ):\n",
    "        return self.b + self.w * x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dummy = ManualLinearRegression()\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ub(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.b = nn.Parameter(torch.randn(1 , requires_grad = True , dtype = torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1 , requires_grad = True  , dtype = torch.float))\n",
    "\n",
    "    def forward(self, x ):\n",
    "        return self.b + self.w * x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dummy = ub()\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## state_dict \n",
    "# curret values of all parameters \n",
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send our dummy model to a device \n",
    "\n",
    "torch.manual_seed(42)\n",
    "dummy = ub().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data \n",
    "x_train_tensor = torch.linspace(1 , 10 , 100 , dtype = torch.float ).to(device)\n",
    "y_train_tensor = x_train_tensor * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([0.7383], device='cuda:0')), ('w', tensor([2.0390], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 \n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters() , lr = lr )\n",
    "\n",
    "#define loss function \n",
    "loss_fn = nn.MSELoss(reduction= 'mean')\n",
    "\n",
    "n_epochs = 100 \n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    #step - 1 \n",
    "    yhat =  model(x_train_tensor)\n",
    "\n",
    "    #step 2 - loss\n",
    "    loss = loss_fn(yhat , y_train_tensor)\n",
    "\n",
    "    #step 3 - computer gradients \n",
    "    loss.backward()\n",
    "\n",
    "    #step - 4 update parameter using its state_dict \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1, out_features=1, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[-0.2191]])), ('bias', tensor([0.2018]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000018A7324D0E0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.7383], device='cuda:0')),\n",
       "             ('w', tensor([2.0390], device='cuda:0'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create  a same model with using linear(a , b) as paremeters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #insed of our custom parameters , we use a linear Model\n",
    "        self.linear = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self  , x  ):\n",
    "        self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.8815]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.7336], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = MyLinearRegression().to(device)\n",
    "list(dummy.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000192016E8900>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[0.7388]], device='cuda:0')),\n",
       "             ('linear.bias', tensor([0.1354], device='cuda:0'))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequential  Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(nn.Linear(1 , 1 )).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential model used when we have to condition like \n",
    "# output of the one model is input to anothers seris of model like simple neural  network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4414,  0.4792, -0.1353],\n",
       "                      [ 0.5304, -0.1265,  0.1165],\n",
       "                      [-0.2811,  0.3391,  0.5090],\n",
       "                      [-0.4236,  0.5018,  0.1081],\n",
       "                      [ 0.4266,  0.0782,  0.2784]], device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')),\n",
       "             ('1.weight',\n",
       "              tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')),\n",
       "             ('1.bias', tensor([-0.2062], device='cuda:0'))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(nn.Linear(3,5) , nn.Linear(5,1)).to(device)\n",
    "model.state_dict()\n",
    "\n",
    "# here 3,5 means  - it is a hidden layer where it takes 3 input and 5 outputs\n",
    "# 5 ,1  is output layer where 5,1 means  , it takes 5 inputs (i.e from previous hidden layer ) and give 1 output i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add module to name the layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Layer 1 ): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (Layer 2 ): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential()\n",
    "model.add_module('Layer 1 ' , nn.Linear(3,5))\n",
    "model.add_module('Layer 2 ' , nn.Linear(5,1))\n",
    "model.to(device)\n",
    "# now we have names of the linear module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
