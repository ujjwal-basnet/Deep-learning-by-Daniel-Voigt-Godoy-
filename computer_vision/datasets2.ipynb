{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, \\\n",
    "precision_recall_curve, auc\n",
    "import os \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i generate_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img , lables = generate_dataset(img_size = 5 , \n",
    "                                n_images = 300 , \n",
    "                                binary = True , \n",
    "                                seed = 13 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300, 1, 5, 5]), torch.Size([300, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data preparation \n",
    "X_tensor  = torch.as_tensor(img / 255).float()\n",
    "y_tensor =  torch.as_tensor(lables.reshape(-1,1)).float()\n",
    "\n",
    "X_tensor.shape , y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation \n",
    "from torch.utils.data import Dataset \n",
    "class TransformTensorDataset(Dataset):\n",
    "    def __init__(self , x , y , transform = None ) :\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self , index):\n",
    "        if self.transform : \n",
    "            x = self.transform(self.x[index])\n",
    "\n",
    "        return x , self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index splitter \n",
    "from torch.utils.data import random_split \n",
    "def index_splitter(n , splits , seed = 13):\n",
    "    idx = torch.arange(n)\n",
    "    split_tensor = torch.as_tensor(splits).float()\n",
    "    multiplier = n / split_tensor.sum()\n",
    "    split_tensor = (multiplier * split_tensor ).long()\n",
    "    diff = n  - split_tensor.sum() \n",
    "    split_tensor[0] = split_tensor[0] + diff \n",
    "    torch.manual_seed(seed)\n",
    "    return random_split(idx , split_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "compose = transforms.Compose([transforms.RandomHorizontalFlip(p = 0.5) , \n",
    "                              transforms.Normalize(mean = (.5) , std = (.5))])\n",
    "\n",
    "dataset  = TransformTensorDataset(X_tensor , y_tensor , transform= compose )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx , val_idx =index_splitter(len(X_tensor), [80 , 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING SETSET DATA SAMPLER \n",
    "from torch.utils.data.sampler  import SubsetRandomSampler\n",
    "train_sampler  = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data loader \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset= dataset , \n",
    "    batch_size = 16 , \n",
    "    sampler = train_sampler \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset= dataset , \n",
    "    batch_size = 16 , \n",
    "    sampler = val_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([240, 1, 5, 5]), torch.Size([240, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using indices to perfrom splits\n",
    "X_train_tensor  = X_tensor[train_idx]\n",
    "y_train_tensor = y_tensor[train_idx] \n",
    "X_val_tensor = X_tensor[val_idx]\n",
    "y_val_tensor = y_tensor[val_idx]\n",
    "\n",
    "X_train_tensor.shape , y_train_tensor.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "train_composer = transforms.Compose([transforms.RandomHorizontalFlip(p = 1) , \n",
    "                                     transforms.Normalize(mean = (.5) , std = (.5))])\n",
    "val_composer = transforms.Compose([transforms.Normalize(mean =(.5)  , std = (.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TransformTensorDataset(X_train_tensor , y_train_tensor , transform = train_composer)\n",
    "\n",
    "val_dataset = TransformTensorDataset(X_train_tensor , y_train_tensor , transform = train_composer)\n",
    "\n",
    "#data loader \n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset , \n",
    "    batch_size = 16 , \n",
    "    shuffle= True\n",
    ") \n",
    "val_loader = DataLoader(\n",
    "    dataset = val_dataset, \n",
    "    batch_size= 16\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.]), tensor([ 80, 160]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes , counts = y_train_tensor.unique(return_counts= True)\n",
    "classes , counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 1 / counts.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0125, 0.0063, 0.0063, 0.0063,\n",
       "        0.0063])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = weights[y_train_tensor.squeeze().long()]\n",
    "sample_weights[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data  import WeightedRandomSampler \n",
    "\n",
    "generator  = torch.Generator()\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights = sample_weights , \n",
    "    num_samples=  len(sample_weights) , \n",
    "    generator = generator , \n",
    "    replacement= True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note that if we are using sampler we cannout use the Shuffle Argument "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the data loaders using the weighted sampler with the training set \n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset , \n",
    "    batch_size = 16 , \n",
    "    sampler = sampler \n",
    ")\n",
    "\n",
    "val_loader =   DataLoader(\n",
    "    dataset = val_dataset , batch_size = 16 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting make_balanced_sampler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_balanced_sampler.py\n",
    "def make_balanced_sampler(y):\n",
    "    classes , counts = y.unique(return_counts = True )\n",
    "    weights = 1.0 / counts.float()\n",
    "    sample_weights = weights[y.squeeze().long()]\n",
    "    #build sampler with compute weights \n",
    "    generator  = torch.Generator()\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights = sample_weights , \n",
    "        num_samples= len(sample_weights), \n",
    "        generator= generator , \n",
    "        replacement= True\n",
    "    )\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i make_balanced_sampler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x241e0ab9e50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = make_balanced_sampler(y_train_tensor)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have it sample a full\n",
    "run (240 data points in 15 mini-batches of 16 points each), and sum up the labels so\n",
    "we know how many points are in the positive class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x241e1c3a610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(10.),\n",
       " tensor(6.),\n",
       " tensor(11.),\n",
       " tensor(7.),\n",
       " tensor(8.),\n",
       " tensor(9.),\n",
       " tensor(9.),\n",
       " tensor(6.),\n",
       " tensor(6.),\n",
       " tensor(10.),\n",
       " tensor(8.),\n",
       " tensor(14.),\n",
       " tensor(7.),\n",
       " tensor(9.),\n",
       " tensor(9.)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([t[1].sum() for t in iter(train_loader)])\n",
    "# here are 15 batches below  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(130.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 240 data points 15 mini batches and 16 points each \n",
    "torch.tensor([t[1].sum()  for t in iter(train_loader)]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous we have 160 images of positive now we ahve 125 only , due to weight sampler , and \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notice we have use two seeds ' but why ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_seed(self , seed = 42):\n",
    "#     torch.backends.cudnn.deterministic = True \n",
    "#     torch.backends.cudnn.benchmark = False \n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "\n",
    "#     try : \n",
    "#         self.train_loader.sampler.generator.manual_seed(seed)\n",
    "#     except AttributeError:\n",
    "#         pass \n",
    "\n",
    "# setattr(StepByStep , 'set_seed' , set_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preParation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.as_tensor(img/255).float()\n",
    "y_tensor  = torch.as_tensor(lables.reshape(-1 , 1)).float()\n",
    "\n",
    "## use index_splitter to split datasets \n",
    "train_idx , val_idx = index_splitter(len(X_tensor ), [80 , 20])\n",
    "#use indices to perform split\n",
    "X_train_tensor = X_tensor[train_idx]\n",
    "y_tensor_tensor = y_tensor[train_idx]\n",
    "X_test_tesnor = X_tensor[val_idx]\n",
    "y_test_tensor = y_tensor[val_idx]\n",
    "\n",
    "####### use composer \n",
    "train_composer = transforms.Compose([transforms.RandomHorizontalFlip(p = 1) , \n",
    "                                     transforms.Normalize(mean =(.5) ,std = (.5))])\n",
    "val_composer = transforms.Compose([transforms.Normalize(mean = (.5) , std = (.5))])\n",
    "\n",
    "######### make dataset now \n",
    "train_dataset = TransformTensorDataset(X_train_tensor , y_train_tensor , transform= train_composer )\n",
    "val_dataset = TransformTensorDataset(y_train_tensor , y_test_tensor , transform = train_composer )\n",
    "\n",
    "## up to here data is transformed \n",
    "\n",
    "## now fixing imbalance classes by giving more weights to lower number of classes  and vice versa\n",
    "sampler = make_balanced_sampler(y_train_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset , batch_size= 16 , sampler = sampler )\n",
    "val_loader = DataLoader(dataset = val_dataset , batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_batch_data , dummy_batch_label = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 5, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_batch_data.shape\n",
    "# 16 images  ,1 channel each of 5 *  5 height and weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 25])\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
      "        -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.])\n"
     ]
    }
   ],
   "source": [
    "#lets flattern \n",
    "flattern = torch.nn.Flatten()\n",
    "dummy_batch_data_flatten = flattern(dummy_batch_data)\n",
    "print(dummy_batch_data_flatten.shape)\n",
    "print(dummy_batch_data_flatten[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14225e94c10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dummy_batch_data[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looses its origina content  👇🏽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14227899410>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dummy_batch_data_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model COnfiguration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 \n",
    "torch.manual_seed(42)\n",
    "#now we can create a model \n",
    "model_logistic = torch.nn.Sequential()\n",
    "model_logistic.add_module('flattern' , nn.Flatten())\n",
    "model_logistic.add_module('output', nn.Linear(25 , 1 , bias = False))\n",
    "model_logistic.add_module('sigmoid' , nn.Sigmoid())\n",
    "## Define an SGD Optimizer to update \n",
    "optimizer_logistic = torch.optim.SGD(\n",
    "    model_logistic.parameters() , lr = lr \n",
    ")\n",
    "\n",
    "#define binary cross entropy\n",
    "binary_loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train our model with 100 epochs \n",
    "n_epochs = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\ujjwal\\\\Deep-learning\\\\computer_vision'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ujjwal\\Deep-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ujjwal\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1.,  1.],\n",
       "           [-1., -1., -1.,  1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [ 1., -1., -1., -1., -1.],\n",
       "           [-1.,  1., -1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1., -1.,  1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1.,  1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1.,  1., -1., -1., -1.],\n",
       "           [ 1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [ 1.,  1.,  1.,  1.,  1.],\n",
       "           [-1., -1., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [ 1.,  1.,  1.,  1.,  1.],\n",
       "           [-1., -1., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [ 1.,  1.,  1.,  1.,  1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [ 1.,  1.,  1.,  1.,  1.],\n",
       "           [-1., -1., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [ 1.,  1.,  1.,  1.,  1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[ 1., -1., -1., -1., -1.],\n",
       "           [-1.,  1., -1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1., -1.,  1., -1.],\n",
       "           [-1., -1., -1., -1.,  1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [ 1., -1., -1., -1., -1.],\n",
       "           [-1.,  1., -1., -1., -1.],\n",
       "           [-1., -1.,  1., -1., -1.],\n",
       "           [-1., -1., -1.,  1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [ 1.,  1.,  1.,  1.,  1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1.,  1.],\n",
       "           [-1., -1., -1.,  1., -1.]]],\n",
       " \n",
       " \n",
       "         [[[-1., -1., -1., -1., -1.],\n",
       "           [ 1.,  1.,  1.,  1.,  1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.],\n",
       "           [-1., -1., -1., -1., -1.]]]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\ujjwal\\\\Deep-learning'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepbystep_v0 import StepByStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(dividend, divisor):\n",
    "\n",
    "    if dividend == 0:\n",
    "        return 0\n",
    "\n",
    "    if divisor == 0:\n",
    "        return None\n",
    "\n",
    "    sign = 1\n",
    "    if dividend < 0 and divisor > 0:\n",
    "        sign = -1\n",
    "    elif dividend > 0 and divisor < 0:\n",
    "                                                sign = -1\n",
    "\n",
    "    dividend = abs(dividend)\n",
    "    divisor = abs(divisor)\n",
    "\n",
    "    res = 0\n",
    "    while dividend >= divisor:\n",
    "        dividend -= divisor\n",
    "        res += 1\n",
    "\n",
    "    return sign * res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divide(10 ,-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x142229e7c90>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor is not a torch image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m sbs_logistic\u001b[38;5;241m=\u001b[39m StepByStep(model \u001b[38;5;241m=\u001b[39m model_logistic  , loss_fn \u001b[38;5;241m=\u001b[39m binary_loss_fn , optimizer\u001b[38;5;241m=\u001b[39m optimizer_logistic)\n\u001b[0;32m      3\u001b[0m sbs_logistic\u001b[38;5;241m.\u001b[39mset_loaders(train_loader , val_loader)\n\u001b[1;32m----> 4\u001b[0m \u001b[43msbs_logistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ujjwal\\Deep-learning\\stepbystep_v0.py:156\u001b[0m, in \u001b[0;36mStepByStep.train\u001b[1;34m(self, n_epochs, seed)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# VALIDATION\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# no gradients in validation!\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Performs evaluation using mini-batches\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_losses\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# If a SummaryWriter has been set...\u001b[39;00m\n",
      "File \u001b[1;32md:\\ujjwal\\Deep-learning\\stepbystep_v0.py:122\u001b[0m, in \u001b[0;36mStepByStep._mini_batch\u001b[1;34m(self, validation)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Once the data loader and step function, this is the same\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# mini-batch loop we had before\u001b[39;00m\n\u001b[0;32m    121\u001b[0m mini_batch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 122\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mTransformTensorDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m , index):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform : \n\u001b[1;32m---> 11\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[index]\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:720\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:684\u001b[0m, in \u001b[0;36mhflip\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mhflip(img)\n\u001b[1;32m--> 684\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:123\u001b[0m, in \u001b[0;36mhflip\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhflip\u001b[39m(img: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 123\u001b[0m     \u001b[43m_assert_image_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\software\\miniconda\\envs\\mlenv\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:15\u001b[0m, in \u001b[0;36m_assert_image_tensor\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_image_tensor\u001b[39m(img: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_tensor_a_torch_image(img):\n\u001b[1;32m---> 15\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor is not a torch image.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Tensor is not a torch image."
     ]
    }
   ],
   "source": [
    "n_epcohs = 100\n",
    "sbs_logistic= StepByStep(model = model_logistic  , loss_fn = binary_loss_fn , optimizer= optimizer_logistic)\n",
    "sbs_logistic.set_loaders(train_loader , val_loader)\n",
    "sbs_logistic.train(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5', '3', '.', '.', '7', '.', '.', '.', '.'],\n",
       " ['6', '.', '.', '1', '9', '5', '.', '.', '.'],\n",
       " ['.', '9', '8', '.', '.', '.', '.', '6', '.'],\n",
       " ['8', '.', '.', '.', '6', '.', '.', '.', '3'],\n",
       " ['4', '.', '.', '8', '.', '3', '.', '.', '1'],\n",
       " ['7', '.', '.', '.', '2', '.', '.', '.', '6'],\n",
       " ['.', '6', '.', '.', '.', '.', '2', '8', '.'],\n",
       " ['.', '.', '.', '4', '1', '9', '.', '.', '5'],\n",
       " ['.', '.', '.', '.', '8', '.', '.', '7', '9']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N  = 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in range(N) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(N):\n",
    "    row = [c for c in board[r] if c!='.']\n",
    "    if len(row)!= len(set(row)) : return False \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not a:\n",
    "    print(\"Fasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fad\n"
     ]
    }
   ],
   "source": [
    "if a :\n",
    "    print(\"fad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(N):\n",
    "    col = [board[r][c] for r in range(N) if board[r][c] !='.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '1', '6', '5', '9']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = set()\n",
    "l.add(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "if 2 in l:\n",
    "    print(\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "6\n",
      "0\n",
      "3\n",
      "6\n",
      "0\n",
      "3\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "l = set()\n",
    "for r in range(0 , 9, 3):\n",
    "    for c in range(0 , 9 , 3):\n",
    "        if board[r][c] in l :\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '3', '.', '.', '7', '.', '.', '.', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board[0][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  r in range(N):\n",
    "    row = [c for c in board[r] if c!='.']\n",
    "    if len(row) != len(set(row)) : \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " for c in range(N):\n",
    "            col = [board[r][c] for r in range(N) if board[r][c] != 0]\n",
    "            if len(col) != len(set(col)): \"false\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n"
     ]
    }
   ],
   "source": [
    "def helper(R , C):\n",
    "            l = set() \n",
    "            for r in range(R , R + 3):\n",
    "                for c in range(C , C + 3):\n",
    "                    if board[r][c] == '.':continue \n",
    "                    if board[r][c] not in l:\n",
    "                        l.add(board[r][c])\n",
    "                    else :\n",
    "                        return False \n",
    "            return True \n",
    "\n",
    "for r in range(0 , N , 3):\n",
    "        for c in range(0 , N , 3):\n",
    "            if not helper(r , c):\"False\"\n",
    "            print(\"fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
